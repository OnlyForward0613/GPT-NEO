{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf500
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\froman\fcharset0 Times-Roman;\f2\froman\fcharset0 Times-Bold;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww25680\viewh14640\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \ul \ulc0 TF Mesh Notes\
\ulnone \
Data Parallelism, normally: \
\
	- Parameters are replicated on every core\
	- Batch Split between cores\
	- Sum parameter gradients\
\
Fast, universal, full utilisation\
\
But doesn\'92t work with giant models, as the model must fit on each device.\
\
TF mesh helps us do this with Model Parallelism, which means we\'92re splitting the model itself between different devices.\
\
Advantages: \
	\
	Can train giant models\
	Low latency (split the computation)\
\
Cons:\
\
	Tricky to design & implement\
\
Normally done with device placement, but that\'92s super hard to design and you end up with giant graphs.\
\
TF Mesh:\
\
	Every processor is involved in every operation\
	Single Program, multiple devices\
	Collective communication\
\
Can supposedly do data parallelism, model parallelism and spatial partitioning.\
\
\
\ul How does it work?\
\ulnone \
For data-parallelism, the data is split over the batch dimension\
\
For Model-parallelism, different dimensions are split. E.G split some dimensions representing sizes of hidden layers\
\
Most operations are local, but some require collective communication (backward pass)\
\
\'91Mesh\'92 comes from the fact that modern supercomputers tend to be built of a mesh of individual processors working in tandem. The mesh is only a naming abstraction and does not imply a physical network topology.\
\
With TFMesh you can even get really fancy by splitting data across one dimension of your 2D mesh, and splitting the model across another.\
\
\pard\pardeftab720\sl300\sa240\partightenfactor0
\cf2 \expnd0\expndtw0\kerning0
\'95 Each tensor in the computation is represented by one (not-necessarily-distinct) slice of the tensor on each processor. \
\'95 Each operation in the computation is implemented as one operation on each processor. Most operations require no communication, with each processor producing its slice of the output from its slices of the inputs. Some operations additionally require collective communication primitives such as MPI-allreduce. \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \kerning1\expnd0\expndtw0 \
\ul General case with TF Mesh:\
\ulnone \
	- Name all tensor-dimensions in the computation\
	- A \'93layout\'94 is a partial map from tensor-dimension-name to mesh-dimension across which that tensor-dimension is split. It guides the program on which tensor dimensions to split across which mesh dimensions E.G: 	layout=\{\'93batch\'94:\'94processor_rows\'94, \'93hidden_size\'94:\'94processor_columns\'94\}\
\
Layouts for example \'93transformer\'94 model:\
\
Data-Parallelism: \{\'93batch\'94:\'94all\'94\}\
Model-Parallelism: \{\'93vocab_size\'94: \'93all\'94, \'93ff_hiddenlayer\'94: \'93all\'94, \'93num_attention_heads\'94: \'93all\'94\}\
Both: \{\'93batch\'94:\'94rows\'94,  \'93vocab_size\'94: \'93columns\'94, \'93ff_hiddenlayer\'94: \'93columns\'94, \'93num_attention_heads\'94: \'93columns\'94\}\
\
\
\ul Picking a good layout is the hard part:\
\ulnone 	\
	- All expensive operations should be split to avoid repeated work\
	- Two dimensions of the same tensor cannot be split across the 	same dimension of the mesh (?)\
	- Slicing the model too finely may result in too much communication 	between cores (i.e batch / hidden layers shouldn\'92t be too small)\
\
\
\ul Using TF Mesh:\
\ulnone \
Build a mesh-tensorflow graph, like a normal tensorflow graph but with named tensor-dimensions. I.E:\
\
	\'93Each Tensor has a static Shape, which is a tuple of different 	"Dimensions". A Dimension is a (name, size) pair. For example, the 	shape of a Tensor representing a batch of images might be:\
	[("batch", 100), ("rows", 28"), ("cols", 28), ("channels", 3)].\'93\
\
\
Define a \'93mesh\'94 i.e an n-dimensional array of processors mapped to your hardware\
\
Define a \'93layout\'94 as explained above\
\
TFMesh turns this into part of your tensor flow graph\
\
\
\ul Example - Two Fully Connected Layers\
\
\pard\pardeftab720\sl300\partightenfactor0
\cf2 \expnd0\expndtw0\kerning0
\ulnone   ...\
  batch = mtf.Dimension("batch", b)\
  io = mtf.Dimension("io", d_io)\
  hidden = mtf.Dimension("hidden", d_h)\
  # x.shape == [batch, io]\
  w = mtf.get_variable("w", shape=[io, hidden])\
  bias = mtf.get_variable("bias", shape=[hidden])\
  v = mtf.get_variable("v", shape=[hidden, io])\
  h = mtf.relu(mtf.einsum(x, w, output_shape=[batch, hidden]) + bias)\
  y = mtf.einsum(h, v, output_shape=[batch, io])\
  ...\cf0 \kerning1\expnd0\expndtw0 \ul \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \ulnone \
\pard\pardeftab720\sl300\sa240\partightenfactor0

\f1\fs26\fsmilli13333 \cf2 \expnd0\expndtw0\kerning0
To train the above model in 
\f2\b data-parallel mode
\f1\b0  on a mesh of n processors, we would define: \
	mesh_shape = [("all", n)] 
\fs24 \
\pard\pardeftab720\sl300\partightenfactor0

\fs26\fsmilli13333 \cf2   	computation_layout = [("batch", "all")]\
\
\pard\pardeftab720\sl300\sa240\partightenfactor0
\cf2 When the Mesh-TensorFlow graph is compiled with this layout, the parameter tensors w, v, and bias are replicated on all processors, but the activation matrices x, h, y, etc. are split across the batch dimension. \
Rather than splitting the batch, we can split the units in the hidden layer: 
\fs24 \
\pard\pardeftab720\sl300\partightenfactor0

\fs26\fsmilli13333 \cf2  	 mesh_shape = [("all", n)]\
 	 computation_layout = [("hidden", "all")]\
\
\pard\pardeftab720\sl300\sa240\partightenfactor0
\cf2 When the Mesh-TensorFlow graph is compiled with this layout, the input and output layers x, and y are replicated on all processors, but the hidden activations h and the parameter tensors w, v and bias are all split across the hidden dimension.\
On a two-dimensional mesh of r \'d7 c processors, we can employ both data-parallelism and model- parallelism: 
\fs24 \
	
\fs26\fsmilli13333 mesh_shape = [("rows", r), ("cols", c)]\
	computation_layout = [("batch", "rows"), ("hidden", "cols")]\
In this layout, each row of processors handles a fraction of the batch, while each column of processors handles a fraction of the hidden units. 
\fs24 \
\pard\pardeftab720\sl300\sa240\partightenfactor0

\f0 \cf0 \kerning1\expnd0\expndtw0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \ul The Mesh TensorFlow Language\
\ulnone \
https://github.com/tensorflow/mesh\
\
\'93Mesh TensorFlow (v0.0) is implemented as a Python library which can generate part of a TensorFlow graph. The user first builds a mtf.Graph (the analog of a TensorFlow graph) made up of mtf.Tensors and mtf.Operations. As in TensorFlow, this graph consists of simple Python objects. The user then creates a mtf.Lowering object, which lowers the mtf.Graph into TensorFlow, adding to the default TensorFlow graph.\'94\
\
\
\ul Basic conversion method from TF to TFMesh:\
\ulnone \
Adapted from TFmesh toy model here: https://github.com/tensorflow/mesh/blob/master/examples/toy_model_tpu.py#L103\
\
	import mesh_tensorflow as mtf\
\
	- Define mesh shape (example for 8 cores): \
		\'93mesh_shape = mtf.convert_to_shape('all:8\'92)\'94\
\
	- Define layout rules (example distributes odd numbered hidden layers across all cores): \
		\'93layout_rules = mtf.convert_to_layout_rules('hidden_odd:all') \'93\
		Layout rules can also be defined automatically using: https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow/auto_mtf\
\
	- Define how vars are placed across the mesh:     \
		\'93var_placer = mtf.utils.BalancedVariablePlacer(device_list,devices_memeory_usage)\'94\
\
	- Name and define dimensions of all tensors, i.e\
		\'93 \
		   batch_dim = mtf.Dimension(\'91batch\'92, batch_size)\
		   io_dim = mtf.Dimension('io', FLAGS.io_size)\'94\
\
	- Define network with TFmesh functions, according to mesh:\
		https://github.com/tensorflow/mesh/blob/master/examples/toy_model_tpu.py#L112\
\
	- Define loss fns with TFmesh functions, I.E: \'93loss = mtf.reduce_mean(mtf.square(y - x))\'94\
\
	- Calc. Gradients, define optimiser and update ops w TFmesh functions: https://github.com/tensorflow/mesh/blob/master/examples/toy_model_tpu.py#L179\
\
	- Use mtf.Lowering to \'91lower\'92 the TFmesh graph into a regular tensor flow graph:\
		\'93lowering = mtf.Lowering(graph, \{mesh: mesh_impl\})\'94\
			\
	- get loss value from distributed loss (?) : \
		tf_loss = tf.to_float(lowering.export_to_tf_tensor(loss))\
\
	- Finalize train op: https://github.com/tensorflow/mesh/blob/master/examples/toy_model_tpu.py#L196\
\
	- Run (regular TF) estimator: \
		\'93tpu_estimator.TPUEstimatorSpec(tf.estimator.ModeKeys.TRAIN,loss=tf_loss,train_op=train_op,training_hooks=[restore_hook, 					saver_hook])\'94\
\
\
\ul Light Reading:\
\ulnone 		\
Transformer implemented in TFmesh: https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow/transformer\
BERT implemented in TFmesh: https://github.com/tensorflow/mesh/tree/8fba074af3c9d823f8b9e901dadd62b0e3085f49/mesh_tensorflow/bert\
TFMesh paper: https://arxiv.org/pdf/1811.02084.pdf\
TFMesh talk: https://www.youtube.com/watch?v=HgGyWS40g-g}